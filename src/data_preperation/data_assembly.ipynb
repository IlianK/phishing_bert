{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import langdetect\n",
    "import numpy as np\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath('../../src'))\n",
    "from helper_functions.path_resolver import DynamicPathResolver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: c:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\phishing_detection\n"
     ]
    }
   ],
   "source": [
    "dpr = DynamicPathResolver(marker=\"README.md\")\n",
    "\n",
    "data_mail_dir = dpr.path.data.raw.data_mail._path\n",
    "\n",
    "test_paths = [\n",
    "    dpr.path.data.raw.data_mail.own.mails_labeled_csv,\n",
    "    dpr.path.data.raw.data_mail.own.jannis_mail_csv,\n",
    "    dpr.path.data.raw.data_mail.curated.Nazario_5_csv,\n",
    "    dpr.path.data.raw.data_mail.curated.SpamAssasin_csv\n",
    "]\n",
    "\n",
    "train_paths = [\n",
    "    dpr.path.data.raw.data_mail.curated.CEAS_08_csv,\n",
    "    dpr.path.data.raw.data_mail.curated.TREC_07_csv\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 20000\n",
    "test_size = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return langdetect.detect(str(text))\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "def add_language_column(df, text_col):\n",
    "    df[\"language\"] = df[text_col].apply(detect_language)\n",
    "    return df\n",
    "\n",
    "def translate_to_de(text):\n",
    "    try:\n",
    "        return GoogleTranslator(source=\"en\", target=\"de\").translate(text)\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    return df.drop_duplicates(subset=[\"subject\", \"body\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_balanced(df, lang, needed_legit, needed_phish):\n",
    "    sub = df[df[\"language\"] == lang]\n",
    "    legit = sub[sub[\"label\"] == 0]\n",
    "    phish = sub[sub[\"label\"] == 1]\n",
    "    legit_samp = legit.sample(n=min(needed_legit, len(legit)), random_state=42)\n",
    "    phish_samp = phish.sample(n=min(needed_phish, len(phish)), random_state=42)\n",
    "    return pd.concat([legit_samp, phish_samp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_german(df, needed_de_legit, needed_de_phish):\n",
    "    current_de_legit = len(df[(df[\"language\"]==\"de\") & (df[\"label\"]==0)])\n",
    "    current_de_phish = len(df[(df[\"language\"]==\"de\") & (df[\"label\"]==1)])\n",
    "    short_legit = needed_de_legit - current_de_legit\n",
    "    short_phish = needed_de_phish - current_de_phish\n",
    "    \n",
    "    if short_legit > 0:\n",
    "        en_legit = df[(df[\"language\"]==\"en\") & (df[\"label\"]==0)]\n",
    "        extra_legit = en_legit.sample(n=min(short_legit, len(en_legit)), random_state=42).copy()\n",
    "        extra_legit[\"subject\"] = extra_legit[\"subject\"].apply(translate_to_de)\n",
    "        extra_legit[\"body\"] = extra_legit[\"body\"].apply(translate_to_de)\n",
    "        extra_legit[\"language\"] = \"de\"\n",
    "        df = pd.concat([df, extra_legit], ignore_index=True)\n",
    "\n",
    "    if short_phish > 0:\n",
    "        en_phish = df[(df[\"language\"]==\"en\") & (df[\"label\"]==1)]\n",
    "        extra_phish = en_phish.sample(n=min(short_phish, len(en_phish)), random_state=42).copy()\n",
    "        extra_phish[\"subject\"] = extra_phish[\"subject\"].apply(translate_to_de)\n",
    "        extra_phish[\"body\"] = extra_phish[\"body\"].apply(translate_to_de)\n",
    "        extra_phish[\"language\"] = \"de\"\n",
    "        df = pd.concat([df, extra_phish], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_test_sets(test_paths, out_dir, total_size=4000):\n",
    "    all_frames = []\n",
    "    for path in test_paths:\n",
    "        df = read_dataset(path)\n",
    "        df = df[df[\"label\"].isin([0,1])]\n",
    "        all_frames.append(df)\n",
    "\n",
    "    combined = pd.concat(all_frames, ignore_index=True)\n",
    "    combined = remove_duplicates(combined)\n",
    "    combined.fillna({\"subject\": \"\", \"body\": \"\"}, inplace=True)\n",
    "    combined = add_language_column(combined, \"body\")\n",
    "\n",
    "    # Split for Phish / Legit for pure EN / DE sets\n",
    "    half = total_size // 2\n",
    "\n",
    "    # English-only \n",
    "    test_raw_en = sample_balanced(combined, \"en\", half, half)\n",
    "\n",
    "    # German-only test set (tranlate)\n",
    "    combined = ensure_german(combined, half, half)\n",
    "    test_raw_de = sample_balanced(combined, \"de\", half, half)\n",
    "\n",
    "    # Mixed set split in Phish / Legit & EN / DE\n",
    "    half_mixed = total_size // 2\n",
    "    quarter = half_mixed // 2\n",
    "\n",
    "    en_half = sample_balanced(combined, \"en\", quarter, quarter)\n",
    "\n",
    "    combined = ensure_german(combined, quarter, quarter)\n",
    "    de_half = sample_balanced(combined, \"de\", quarter, quarter)\n",
    "\n",
    "    test_raw_en_de = pd.concat([en_half, de_half], ignore_index=True)\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    test_raw_en.to_csv(os.path.join(out_dir, \"test_raw_en.csv\"), index=False)\n",
    "    test_raw_de.to_csv(os.path.join(out_dir, \"test_raw_de.csv\"), index=False)\n",
    "    test_raw_en_de.to_csv(os.path.join(out_dir, \"test_raw_en_de.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_test_sets(test_paths, data_mail_dir, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_sets(train_paths, out_dir, total_size=20000):\n",
    "    all_frames = []\n",
    "    for path in train_paths:\n",
    "        df = read_dataset(path)\n",
    "        df = df[df[\"label\"].isin([0,1])]\n",
    "        all_frames.append(df)\n",
    "\n",
    "    combined = pd.concat(all_frames, ignore_index=True)\n",
    "    combined = remove_duplicates(combined)\n",
    "    combined.fillna({\"subject\": \"\", \"body\": \"\"}, inplace=True)\n",
    "    combined = add_language_column(combined, \"body\")\n",
    "\n",
    "    # Split in Legit / Phish\n",
    "    half = total_size // 2  # total legit or total phish\n",
    "    combined = ensure_german(combined, half, half)\n",
    "\n",
    "    # Split again for EN / DE\n",
    "    quarter = half // 2\n",
    "\n",
    "    # Legit & EN\n",
    "    en_legit = sample_balanced(combined, \"en\", quarter, 0)\n",
    "    en_legit = en_legit[en_legit[\"label\"] == 0].sample(n=min(quarter, len(en_legit)), random_state=42)\n",
    "\n",
    "    # Phish & EN\n",
    "    en_phish = sample_balanced(combined, \"en\", 0, quarter)\n",
    "    en_phish = en_phish[en_phish[\"label\"] == 1].sample(n=min(quarter, len(en_phish)), random_state=42)\n",
    "\n",
    "    # Legit & DE\n",
    "    de_legit = sample_balanced(combined, \"de\", quarter, 0)\n",
    "    de_legit = de_legit[de_legit[\"label\"] == 0].sample(n=min(quarter, len(de_legit)), random_state=42)\n",
    "\n",
    "    # Phish & DE\n",
    "    de_phish = sample_balanced(combined, \"de\", 0, quarter)\n",
    "    de_phish = de_phish[de_phish[\"label\"] == 1].sample(n=min(quarter, len(de_phish)), random_state=42)\n",
    "\n",
    "    train_final = pd.concat([en_legit, en_phish, de_legit, de_phish], ignore_index=True)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    train_final.to_csv(os.path.join(out_dir, \"train_raw_balanced.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_train_sets(train_paths, data_mail_dir, train_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifiy Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(df, name):\n",
    "    print(f\"\\n{name}, Rows: {len(df)}\")\n",
    "    print(\"----------------------------------------\")\n",
    "\n",
    "    class_counts = df[\"label\"].value_counts().to_dict()\n",
    "    lang_counts = df[\"language\"].value_counts().to_dict()\n",
    "    grouped = df.groupby([\"label\", \"language\"]).size().to_dict()\n",
    "\n",
    "    print(f\"Class Distribution: {class_counts}\")\n",
    "    print(f\"Language Distribution: {lang_counts}\")\n",
    "    print(f\"Detailed (Class, Language) Distribution: {grouped}\")\n",
    "    print(\"----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Balanced, Rows: 20000\n",
      "----------------------------------------\n",
      "Class Distribution: {0: 10000, 1: 10000}\n",
      "Language Distribution: {'en': 10000, 'de': 10000}\n",
      "Detailed (Class, Language) Distribution: {(0, 'de'): 5000, (0, 'en'): 5000, (1, 'de'): 5000, (1, 'en'): 5000}\n",
      "----------------------------------------\n",
      "\n",
      "Test EN, Rows: 4000\n",
      "----------------------------------------\n",
      "Class Distribution: {0: 2000, 1: 2000}\n",
      "Language Distribution: {'en': 4000}\n",
      "Detailed (Class, Language) Distribution: {(0, 'en'): 2000, (1, 'en'): 2000}\n",
      "----------------------------------------\n",
      "\n",
      "Test DE, Rows: 4000\n",
      "----------------------------------------\n",
      "Class Distribution: {0: 2000, 1: 2000}\n",
      "Language Distribution: {'de': 4000}\n",
      "Detailed (Class, Language) Distribution: {(0, 'de'): 2000, (1, 'de'): 2000}\n",
      "----------------------------------------\n",
      "\n",
      "Test Mixed, Rows: 4000\n",
      "----------------------------------------\n",
      "Class Distribution: {0: 2000, 1: 2000}\n",
      "Language Distribution: {'en': 2000, 'de': 2000}\n",
      "Detailed (Class, Language) Distribution: {(0, 'de'): 1000, (0, 'en'): 1000, (1, 'de'): 1000, (1, 'en'): 1000}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(os.path.join(data_mail_dir, \"train_raw_balanced.csv\"))\n",
    "verify(df_train, \"Train Balanced\")\n",
    "\n",
    "df_test_en = pd.read_csv(os.path.join(data_mail_dir, \"test_raw_en.csv\"))\n",
    "verify(df_test_en, \"Test EN\")\n",
    "\n",
    "df_test_de = pd.read_csv(os.path.join(data_mail_dir, \"test_raw_de.csv\"))\n",
    "verify(df_test_de, \"Test DE\")\n",
    "\n",
    "df_test_mixed = pd.read_csv(os.path.join(data_mail_dir, \"test_raw_en_de.csv\"))\n",
    "verify(df_test_mixed, \"Test Mixed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
