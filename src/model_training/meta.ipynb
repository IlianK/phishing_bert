{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ilian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)     \n",
    "    random.seed(seed)             \n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  \n",
    "    \n",
    "    # TensorFlow\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "\n",
    "set_seed(42)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: c:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\phishing_detection\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../../src'))\n",
    "from helper_functions.path_resolver import DynamicPathResolver\n",
    "\n",
    "dpr = DynamicPathResolver(marker=\"README.md\")\n",
    "paths = dpr.structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = paths.data.raw.data_mail.train_raw_csv\n",
    "test_raw = paths.data.raw.data_mail.test_raw_csv\n",
    "\n",
    "train_preprocessed = paths.data.preprocessed.data_mail.train_processed_meta_csv\n",
    "test_preprocessed = paths.data.preprocessed.data_mail.test_processed_meta_csv\n",
    "\n",
    "models_folder = dpr.get_folder_path_from_namespace(paths.models.meta)\n",
    "output_dir = dpr.get_folder_path_from_namespace(paths.models.meta.results)\n",
    "\n",
    "os.makedirs(models_folder, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data(train_load_path, test_load_path):\n",
    "    df_train = pd.read_csv(train_load_path)\n",
    "    df_test = pd.read_csv(test_load_path)\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    model_path = os.path.join(models_folder, f\"{model_name.replace(' ', '_').lower()}.pkl\")\n",
    "    return joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_scaling(X_train, X_val, scale=False):\n",
    "    if scale:\n",
    "        scaler = StandardScaler() # StandardScaler # MinMaxScaler\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        return X_train_scaled, X_val_scaled\n",
    "    return X_train, X_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, test_file, models_folder):\n",
    "    # Load preprocessed\n",
    "    df_test = pd.read_csv(test_file)\n",
    "    y_test = df_test['label']\n",
    "    X_test = df_test.drop(columns=['label'])\n",
    "\n",
    "    # Load model\n",
    "    model = load_model(model_name)\n",
    "\n",
    "    # Load vectorizer\n",
    "    vectorizer_path = os.path.join(models_folder, \"tfidf_vectorizer.pkl\")\n",
    "    vectorizer = joblib.load(vectorizer_path)\n",
    "\n",
    "    # Extract text & numerical\n",
    "    X_test_text = X_test['body']\n",
    "    X_test_numerical = X_test.drop(columns=['body'])\n",
    "\n",
    "    # Vectorize\n",
    "    X_test_tfidf = vectorizer.transform(X_test_text)\n",
    "\n",
    "    # Scale numericals\n",
    "    if model_name in [\"log_regression\"]:\n",
    "        _, X_test_numerical_scaled = conditional_scaling(X_test_numerical, X_test_numerical, scale=True)\n",
    "    else:\n",
    "        _, X_test_numerical_scaled = conditional_scaling(X_test_numerical, X_test_numerical, scale=False)\n",
    "\n",
    "    # Combine and pred\n",
    "    X_test_combined = np.hstack([X_test_tfidf.toarray(), X_test_numerical_scaled])\n",
    "    y_pred = model.predict(X_test_combined)\n",
    "\n",
    "    # Classification report\n",
    "    print(f\"\\nEvaluation for {model_name}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "    display(report_df)\n",
    "\n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Ham', 'Phishing'], yticklabels=['Ham', 'Phishing'])\n",
    "    plt.xlabel(\"Prediction\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = load_preprocessed_data(train_preprocessed, test_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(columns=['label'])\n",
    "y = df_train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = X_train['body']\n",
    "X_val_text = X_val['body']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numerical = X_train.drop(columns=['body'])\n",
    "X_val_numerical = X_val.drop(columns=['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unscaled, X_val_unscaled = conditional_scaling(X_train_numerical, X_val_numerical, scale=False)\n",
    "X_train_scaled, X_val_scaled = conditional_scaling(X_train_numerical, X_val_numerical, scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) | set(stopwords.words('german')) \n",
    "stop_words_list = list(stop_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\ilian\\\\Documents\\\\Projects\\\\git_projects\\\\university\\\\phishing_detection\\\\models\\\\meta\\\\tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words_list, max_features=5000)\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_text)\n",
    "X_val_tfidf = vectorizer.transform(X_val_text)\n",
    "\n",
    "joblib.dump(vectorizer, os.path.join(models_folder, \"tfidf_vectorizer.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack features (TF-IDF & Custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined_unscaled = np.hstack([X_train_tfidf.toarray(), X_train_unscaled])\n",
    "X_val_combined_unscaled = np.hstack([X_val_tfidf.toarray(), X_val_unscaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined_scaled = np.hstack([X_train_tfidf.toarray(), X_train_scaled])\n",
    "X_val_combined_scaled = np.hstack([X_val_tfidf.toarray(), X_val_scaled])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"naive_bayes\": MultinomialNB(),\n",
    "    \"log_regression\": LogisticRegression(max_iter=500),\n",
    "    \"decision_tree\": DecisionTreeClassifier(),\n",
    "    \"random_forest\": RandomForestClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Naive Bayes params: {'alpha': 0.01}\n"
     ]
    }
   ],
   "source": [
    "param_grid_nb = {'alpha': [0.01, 0.1, 0.5, 1, 2, 5]}\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "grid_search_nb = GridSearchCV(nb, param_grid_nb, cv=5, n_jobs=-1)\n",
    "grid_search_nb.fit(X_train_combined_unscaled, y_train)\n",
    "\n",
    "best_nb = grid_search_nb.best_estimator_\n",
    "\n",
    "joblib.dump(best_nb, os.path.join(models_folder, \"naive_bayes.pkl\"))\n",
    "print(\"Best Naive Bayes params:\", grid_search_nb.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_lr = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100], \n",
    "    'solver': ['liblinear', 'saga'], \n",
    "    'max_iter': [200, 500, 1000]\n",
    "    }\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "grid_search_lr = GridSearchCV(lr, param_grid_lr, cv=5, n_jobs=-1)\n",
    "grid_search_lr.fit(X_train_combined_scaled, y_train)\n",
    "\n",
    "best_lr = grid_search_lr.best_estimator_\n",
    "\n",
    "joblib.dump(best_lr, os.path.join(models_folder, \"log_regression.pkl\"))\n",
    "print(\"Best Logistic Regression params:\", grid_search_lr.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_dt = {\n",
    "    'max_depth': [None, 10, 20, 30], \n",
    "    'min_samples_split': [2, 5, 10], \n",
    "    'min_samples_leaf': [1, 2, 4], \n",
    "    'criterion': ['gini', 'entropy']\n",
    "    }\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "grid_search_dt = GridSearchCV(dt, param_grid_dt, cv=5, n_jobs=-1)\n",
    "grid_search_dt.fit(X_train_combined_unscaled, y_train)\n",
    "\n",
    "best_dt = grid_search_dt.best_estimator_\n",
    "joblib.dump(best_dt, os.path.join(models_folder, \"decision_tree.pkl\"))\n",
    "\n",
    "print(\"Best Decision Tree params:\", grid_search_dt.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 500], \n",
    "    'max_depth': [None, 10, 20, 30], \n",
    "    'min_samples_split': [2, 5, 10], \n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, n_jobs=-1)\n",
    "grid_search_rf.fit(X_train_combined_unscaled, y_train)\n",
    "\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "joblib.dump(best_rf, os.path.join(models_folder, \"random_forest.pkl\"))\n",
    "print(\"Best Random Forest params:\", grid_search_rf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in models.keys():\n",
    "    evaluate_model(name, test_preprocessed, models_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_nb = joblib.load(os.path.join(models_folder, \"naive_bayes.pkl\"))\n",
    "best_lr = joblib.load(os.path.join(models_folder, \"log_regression.pkl\"))\n",
    "best_dt = joblib.load(os.path.join(models_folder, \"decision_tree.pkl\"))\n",
    "best_rf = joblib.load(os.path.join(models_folder, \"random_forest.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('naive_bayes', best_nb),\n",
    "    #('log_regression', best_lr),\n",
    "    ('decision_tree', best_dt),\n",
    "    ('random_forest', best_rf)\n",
    "], voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf.fit(X_train_combined_unscaled, y_train)\n",
    "joblib.dump(voting_clf, os.path.join(models_folder, \"ensemble_model.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\"ensemble_model\", test_preprocessed, models_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
